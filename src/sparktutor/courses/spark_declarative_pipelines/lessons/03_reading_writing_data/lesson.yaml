- Class: meta
  Lesson: "Reading & Writing Data"
  EstimatedMinutes: 15

- Class: text
  Depth: all
  Output: |
    A pipeline is only useful if it can **ingest** data from the outside
    world and **persist** results. Spark's DataFrameReader (`spark.read`)
    and DataFrameWriter (`df.write`) support dozens of formats out of the
    box: CSV, JSON, Parquet, ORC, Avro, and more.

    The general pattern:
    ```python
    # Read
    df = spark.read.format("csv").option("header", "true").load("/data/input")

    # Write
    df.write.format("parquet").mode("overwrite").save("/data/output")
    ```

- Class: text
  Depth: beginner
  Output: |
    The simplest way to read a CSV file:

    ```python
    df = spark.read.csv("/data/sales.csv", header=True, inferSchema=True)
    ```

    `header=True` tells Spark the first row contains column names.
    `inferSchema=True` makes Spark scan the file to guess column types
    (string, integer, double, etc.). This is convenient for exploration
    but slow on large files.

- Class: mult_question
  Depth: all
  Output: "What does `inferSchema=True` do when reading a CSV?"
  AnswerChoices: "Scans the data to guess column types automatically;Enforces a user-provided schema;Skips rows with bad data;Infers the file encoding"
  CorrectAnswer: "Scans the data to guess column types automatically"
  Hint: "Spark needs an extra pass over the data to figure out the types."

- Class: cmd_question
  Depth: all
  Output: |
    Read a CSV file at '/data/orders.csv' with headers and inferred schema
    into a DataFrame called `orders`.
  CorrectAnswer: |
    orders = spark.read.csv("/data/orders.csv", header=True, inferSchema=True)
  Hint: "spark.read.csv(path, header=True, inferSchema=True)"

- Class: text
  Depth: intermediate
  Output: |
    For production pipelines you should **never** rely on `inferSchema`.
    Instead, define the schema explicitly using `StructType`:

    ```python
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType

    schema = StructType([
        StructField("order_id", StringType(), nullable=False),
        StructField("product", StringType()),
        StructField("price", DoubleType()),
        StructField("quantity", IntegerType()),
    ])

    df = spark.read.csv("/data/orders.csv", header=True, schema=schema)
    ```

    This is faster (no extra scan) and safer (bad data becomes `null`
    instead of silently mistyped).

- Class: cmd_question
  Depth: intermediate
  Output: |
    Define a schema for a `users` CSV with columns: `user_id` (StringType),
    `name` (StringType), and `age` (IntegerType). Then read '/data/users.csv'
    using that schema.
  CorrectAnswer: |
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType

    schema = StructType([
        StructField("user_id", StringType()),
        StructField("name", StringType()),
        StructField("age", IntegerType()),
    ])

    users = spark.read.csv("/data/users.csv", header=True, schema=schema)
  Hint: "Build a StructType with three StructField entries, then pass schema=schema to spark.read.csv."

- Class: text
  Depth: all
  Output: |
    **Write modes** control what happens when the output path already exists:

    | Mode        | Behavior                              |
    |-------------|---------------------------------------|
    | `overwrite` | Delete existing data, write new       |
    | `append`    | Add new data alongside existing       |
    | `error`     | Fail if data already exists (default) |
    | `ignore`    | Silently skip if data exists          |

- Class: mult_question
  Depth: all
  Output: "What is the default write mode if you do not specify one?"
  AnswerChoices: "error;overwrite;append;ignore"
  CorrectAnswer: "error"
  Hint: "Spark is cautious by default -- it will not overwrite existing data unless you tell it to."

- Class: cmd_question
  Depth: all
  Output: |
    Write the DataFrame `orders` to Parquet format at '/output/orders_parquet'
    using overwrite mode.
  CorrectAnswer: |
    orders.write.mode("overwrite").parquet("/output/orders_parquet")
  Hint: "df.write.mode('overwrite').parquet(path)"

- Class: text
  Depth: intermediate
  Output: |
    **Parquet** is the preferred format for analytics pipelines because:
    - It is columnar (reads only the columns you need)
    - It compresses well (snappy by default)
    - It preserves the schema inside the file (no inference needed)

    Reading Parquet is as simple as:
    ```python
    df = spark.read.parquet("/data/orders.parquet")
    ```

- Class: cmd_question
  Depth: intermediate
  Output: |
    Read a Parquet file at '/data/events.parquet', filter to rows where
    `event_type` equals 'purchase', and write the result as Parquet to
    '/output/purchases' in overwrite mode.
  CorrectAnswer: |
    from pyspark.sql import functions as f
    events = spark.read.parquet("/data/events.parquet")
    purchases = events.filter(f.col("event_type") == "purchase")
    purchases.write.mode("overwrite").parquet("/output/purchases")
  Hint: "Chain read -> filter -> write. Use f.col for the filter condition."

- Class: text
  Depth: advanced
  Output: |
    **Iceberg** is a table format that adds ACID transactions, time travel,
    and partition evolution on top of Parquet files. In Spark 4.1 you write
    to Iceberg tables like this:

    ```python
    df.writeTo("lakehouse.db.orders").using("iceberg").createOrReplace()
    ```

    Key benefits:
    - **Partition pruning** -- Iceberg maintains partition statistics so
      Spark skips entire files that cannot match your filter.
    - **Schema evolution** -- add, rename, or drop columns without rewriting
      data.
    - **Time travel** -- query the table as it was at a previous snapshot.

    ```python
    spark.read.option("snapshot-id", 12345).table("lakehouse.db.orders")
    ```

- Class: mult_question
  Depth: advanced
  Output: "Which Iceberg feature lets you query a table as it looked at a previous point in time?"
  AnswerChoices: "Time travel via snapshot IDs;Schema evolution;Partition pruning;Compaction"
  CorrectAnswer: "Time travel via snapshot IDs"
  Hint: "Iceberg stores immutable snapshots; you can read any historical snapshot."

- Class: cmd_question
  Depth: advanced
  Output: |
    Write DataFrame `orders` to an Iceberg table called
    'lakehouse.bronze.orders' using createOrReplace.
  CorrectAnswer: |
    orders.writeTo("lakehouse.bronze.orders").using("iceberg").createOrReplace()
  Hint: "Use df.writeTo(table_name).using('iceberg').createOrReplace()"

- Class: text
  Depth: all
  Output: |
    You can now read and write data in multiple formats with explicit
    schemas. Next up: building a **Pipeline Framework** that ties your
    transforms together into a declarative, dependency-aware workflow.
