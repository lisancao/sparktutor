- Class: meta
  Lesson: "Gold Layer"
  EstimatedMinutes: 15

- Class: text
  Depth: all
  Output: |
    The **gold layer** produces business-ready aggregations. While bronze
    preserves raw data and silver cleans it, gold answers specific business
    questions:

    - "What is the total revenue per product?"
    - "How many orders per hour?"
    - "What is the running total of sales this month?"

    Gold tables are typically consumed by dashboards, reports, and ML
    feature stores.

- Class: text
  Depth: beginner
  Output: |
    The workhorse of the gold layer is `groupBy().agg()`:

    ```python
    from pyspark.sql import functions as f

    gold = (silver
        .groupBy("product")
        .agg(
            f.count("*").alias("order_count"),
            f.sum("total").alias("revenue"),
            f.avg("total").alias("avg_order_value"),
        )
    )
    ```

    `alias()` gives each aggregated column a meaningful name.

- Class: mult_question
  Depth: all
  Output: "Which method do you call after groupBy() to compute aggregations?"
  AnswerChoices: "agg();compute();reduce();aggregate()"
  CorrectAnswer: "agg()"
  Hint: "It is a short three-letter method that accepts one or more aggregate expressions."

- Class: cmd_question
  Depth: all
  Output: |
    Given a silver DataFrame `silver_df` with columns `product`, `quantity`,
    and `total`, write a groupBy that computes the total `quantity` sold
    and total `revenue` (sum of `total`) per `product`.
  CorrectAnswer: |
    from pyspark.sql import functions as f
    gold_df = (silver_df
        .groupBy("product")
        .agg(
            f.sum("quantity").alias("total_qty"),
            f.sum("total").alias("revenue"),
        )
    )
  Hint: "Use groupBy('product').agg(f.sum(...).alias(...), f.sum(...).alias(...))"

- Class: text
  Depth: all
  Output: |
    Common aggregate functions:

    | Function              | Description                    |
    |-----------------------|--------------------------------|
    | `f.count("*")`        | Number of rows                 |
    | `f.sum("col")`        | Sum of values                  |
    | `f.avg("col")`        | Average (mean)                 |
    | `f.min("col")`        | Minimum value                  |
    | `f.max("col")`        | Maximum value                  |
    | `f.countDistinct("c")`| Number of unique values        |

- Class: mult_question
  Depth: all
  Output: "Which function counts the number of unique values in a column?"
  AnswerChoices: "f.countDistinct();f.count();f.uniqueCount();f.distinct()"
  CorrectAnswer: "f.countDistinct()"
  Hint: "It combines 'count' and 'distinct' into one function name."

- Class: cmd_question
  Depth: intermediate
  Output: |
    Compute an hourly summary from `silver_df`: group by `order_hour` and
    calculate `order_count` (count of rows), `total_revenue` (sum of total),
    and `unique_products` (count distinct of product). Sort by `order_hour`.
  CorrectAnswer: |
    from pyspark.sql import functions as f
    hourly = (silver_df
        .groupBy("order_hour")
        .agg(
            f.count("*").alias("order_count"),
            f.sum("total").alias("total_revenue"),
            f.countDistinct("product").alias("unique_products"),
        )
        .orderBy("order_hour")
    )
  Hint: "groupBy order_hour, agg with count, sum, countDistinct, then orderBy."

- Class: text
  Depth: intermediate
  Output: |
    **Window functions** compute values across a set of rows related to the
    current row, without collapsing them into a single row like groupBy does.

    Common use case: running totals, rankings, moving averages.

    ```python
    from pyspark.sql import Window

    w = Window.partitionBy("product").orderBy("order_date").rowsBetween(
        Window.unboundedPreceding, Window.currentRow
    )

    df = df.withColumn("running_revenue", f.sum("total").over(w))
    ```

    This computes a cumulative sum of `total` for each product, ordered
    by date.

- Class: cmd_question
  Depth: intermediate
  Output: |
    Add a `running_total` column to `silver_df` that is the cumulative sum
    of `total` partitioned by `product` and ordered by `order_ts`.
  CorrectAnswer: |
    from pyspark.sql import functions as f
    from pyspark.sql import Window

    w = Window.partitionBy("product").orderBy("order_ts").rowsBetween(
        Window.unboundedPreceding, Window.currentRow
    )
    result = silver_df.withColumn("running_total", f.sum("total").over(w))
  Hint: "Define a Window with partitionBy, orderBy, and rowsBetween. Use f.sum().over(window)."

- Class: text
  Depth: advanced
  Output: |
    **Performance tuning** for gold aggregations:

    1. **Reduce shuffle partitions**: for small to medium datasets, set
       `spark.sql.shuffle.partitions` to 8-32 instead of the default 200.

    2. **Cache intermediate results**: if a silver DataFrame is used by
       multiple gold aggregations, cache it first:
       ```python
       silver_df.cache()
       silver_df.count()  # materialize the cache
       ```

    3. **Repartition before groupBy**: if data is heavily skewed, repartition
       on the groupBy key first:
       ```python
       silver_df.repartition("product").groupBy("product").agg(...)
       ```

    4. **Two-phase aggregation**: for extremely skewed keys, add a random
       salt, aggregate partially, then aggregate again without the salt.

- Class: mult_question
  Depth: advanced
  Output: "What is the purpose of two-phase (salted) aggregation?"
  AnswerChoices: "To handle data skew by distributing hot keys across partitions;To speed up I/O;To reduce memory usage;To improve join performance"
  CorrectAnswer: "To handle data skew by distributing hot keys across partitions"
  Hint: "Adding a random salt breaks up a single hot key into multiple sub-keys for the first phase."

- Class: cmd_question
  Depth: advanced
  Output: |
    Implement a two-phase aggregation for summing `total` by `product`.
    Phase 1: add a salt column (random int 0-9), groupBy product + salt,
    sum total. Phase 2: drop salt, groupBy product, sum again.
  CorrectAnswer: |
    from pyspark.sql import functions as f

    # Phase 1: salted partial aggregation
    salted = silver_df.withColumn("_salt", (f.rand() * 10).cast("int"))
    partial = (salted
        .groupBy("product", "_salt")
        .agg(f.sum("total").alias("partial_revenue"))
    )

    # Phase 2: final aggregation without salt
    final = (partial
        .groupBy("product")
        .agg(f.sum("partial_revenue").alias("revenue"))
    )
  Hint: "Add a random _salt column, groupBy(product, _salt) for phase 1, then groupBy(product) for phase 2."

- Class: script
  Depth: all
  Output: |
    Build a complete gold aggregation. Open `starter.py` and implement
    `gold_product_summary` that:
    1. Reads from the `silver_orders` temp view
    2. Groups by `product`
    3. Computes `order_count`, `total_revenue`, `avg_order_value`
    4. Adds a `revenue_rank` column using a window function (rank by total_revenue desc)
  StarterFile: starter.py
  SolutionFile: solution.py
  Hint: "groupBy product, agg for counts and sums, then use Window with f.rank().over(w)."

- Class: text
  Depth: all
  Output: |
    Your gold layer now produces clean, aggregated, business-ready data.
    In the final lesson you will connect all three layers into a single
    end-to-end pipeline.
